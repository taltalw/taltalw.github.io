---
title: 'Paper reading 1'
date: 2024-01-02
permalink: /posts/2024/01/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---
g
# 论文阅读

### Can Small Language Models be Good Reasoners for Sequential Recommendation?

- motivation: 序列化推荐往往依赖于用户的交互数据，而没有关注到导致用户行为的原因。利用CoT的推理能力，实现有依据的序列化推荐。
- novelty: 通过CoT充分分析出交互数据中的隐含信息，通过fine-tuned SLM加速推理。
- method:
  - LLM: Teacher model, 通过CoT提示逐步生成推荐的rationals
  - SLM: Student model, 用LLM生成的结果进行微调，微调后的小模型作为最终的=生成器，为推荐系统提供推理知识。
  - 最后通过TextEncoder将item描述和rationals映射到一个向量空间。

### Large Language Models are Learnable Planners for Long-Term Recommendation

- motivation: 基于强化学习的long-term recommendation方法数据需求，能否不依赖于学习数据进行long-term recommendation
- novelty: 调用LLM，它本身就有训练数据，所以不需要额外的数据再去训练。用LLM的规划能力生成推荐的计划，实时注入用户交互的历史交互信息用于下一轮推荐的决策。
- method:
  - macro-learning: 用于生成计划，reflection指导计划的不断优化。
  - micro-learning: 用于产生计划所导致的行为，用强化学习的ACTOR-CRITIC框架，通过计算reward指导行为的不断优化。
![img.png](image/BiLLP/img.png)

### CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following

- motivation: 用户的隐私保护和模型的表现难以平衡，如何保护用户隐私又有效地执行指令？
- novelty: 综合考虑LLM的high performance和SLM的隐私性。将用户画像处理为Personalized Content，用于SLM的微调和LLM-SLM协同的个性化适配。
- method:
    - 用 LLM 搭建整体回复的框架（general instruction）
    - 依靠personalized content，微调 SLM，使回复个性化（personal instruction）
    - SLM 和 LLM的回复相结合
      ![img_2.png](image/CoGenesis/img_2.png)
    - p.s. personalized content的获取需要经过四步的Data Construction


### An Emulator for Fine-Tuning Large Language Models using Small Language Models

- motivation: 探究微调和与训练两阶段对模型的具体贡献
- novelty: EFT框架能够解耦预训练和微调的效果，从而评估每个阶段的影响。up-scaling结合大模型与训练和小模型微调，避免了直接微调大型模型所需的高昂计算成本。
- method:
  - 每个推理步骤的概率计算
        ![img.png](image/EFT/img.png)
  - 实验时 对整体进行采样
        ![img_1.png](image/EFT/img_1.png)
  - up-scaling: N >> M

### Tuning Language Models by Proxy

- motivation: 模型适应特定的场景需要微调，微调开源模型需要大量的算力资源，闭源模型则无法直接微调。
- novelty: 提出一种proxy-tuning的方法，微调SLM，根据SLM的输出变化预测LLM的输出，而无需对LLM的参数进行微调。
- method: ![img_6.png](image/proxy-tuning/img_6.png)

### AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

- 创新点：提出了一种多agent协作的框架，重写接口即可定制个性化agent。

    e.g. 官方文档给的例子 *get weather*
    ![img.png](image/AutoGen/img.png)
- AutoGen框架：以一个conversation agent为核心，手动封装其他agent的接口，供conversation agent调用，如图所示。
    ![img_1.png](image/AutoGen/img_1.png)
- 思考：从原文的几个例子来看，这个框架只是实现了接口的自动化，就是说我设计好了几个agent不用关他们互相之间是什么样的调用关系，且不同任务的agent都不一样。
    ！[img_2.png](image/AutoGen/img_2.png)

### Graph Retrieval-Augmented Generation: A Survey
- 主要看一下graph RAG的原理：通过引入图结构，检索实体及其关系，确保生成内容反映实体间的联系，如知识图谱中的节点和边。
- LLM vs RAG vs GRAPHRAG
![img.png](image/GraphRAG/img3.png)
    Direct LLM：仅依赖模型生成，容易缺乏具体细节。 

    RAG：检索外部文本，但处理复杂关系能力有限。

    GraphRAG：通过知识图谱检索，确保答案准确且全面。
- limitation: 知识图谱覆盖度不足或更新滞后; 在多跳推理和复杂任务中，GraphRAG计算开销大; 对于开放性或创造性任，GraphRAG表现不如直接生成模型，灵活性较弱.

### FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance

![img.png](image/FrugalGPT/img4.png)

- 按顺序向LLM发送查询，满足评分，则返回，否则继续查询直到满足评分。
- 关键点：大模型的选择和顺序的设置
- limitation: 成本高、速度慢
- 思考：可以借鉴的地方是，如果遇到差不多的LLM，可以选择成本最低的LLM作为贪心策略

### RouteLLM: Learning to Route LLMs with Preference Data

采用胜率预测模型，预测较强模型获胜的概率，根据算出的胜率根据阈值进行路由决策。

关键在于胜率的计算，提出了四种路由策略计算胜率。

- Similarity-weighted (SW) ranking：本质上是相似度计算，根据相似查询的原始输出进行预测。
    ![img.png](image/RouteLLM/img.png)
- Matrix factorization：矩阵分解模型在推荐系统中被广泛使用，以捕捉用户-物品交互的低秩结构。这里借鉴了这一方法，应用于偏好数据的训练。
    ![img_1.png](image/RouteLLM/img_1.png)
- BERT classifier：探索使用标准文本分类方法，使用BERT-base架构，为用户查询提供上下文化嵌入
    ![img_2.png](image/RouteLLM/img_2.png)
- Causal LLM classifier：使用Llama3-8B参数化路由器，构造instruction-following范式进行训练

### ROUTERBENCH: A Benchmark for Multi-LLM Routing System

提出一个用于评估多LLM路由系统的基准测试框架。

- 评价标准：AIQ
    ![img_3.png](image/ROUTERBENCH/img_3.png)
- 方法：插值、非递减凸包构造
    ![img_5.png](image/ROUTERBENCH/img_5.png)
- 用这个评价标准评价下来发现非预测路由（FrugalGPT）比预测路由（KNN, MLP）效果好得多，但我认为这个非预测路由的意义不大，因为级联调用大模型的时间成本太高了，不能说是先调“小模型”减少成本就算做整个路由都优化成功了。
- 比较有参考价值的实验结果是RAG数据集的AIQ非常高，包括非预测路由和预测路由，由此想到能不能设置一个双阶段路由器。
