---
title: 'Paper reading 3'
date: 2024-01-22
permalink: /posts/2024/01/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models

- motivation: 微调LLM是通过“上课”获取知识（模型局限、算力局限），RAG是通过搜索获取知识（推理能力受限、噪声大）。能否结合二者的长处，通过一个具有特定知识的“小专家”获取知识，这样既具备一定的推理能力，又解决了直接与训练LLM的局限性。

- novelty: 
  - domain knowledge的获取。通过prompt让LLM生成它想要什么knowledge，然后再让SLM去找。

  ![img.png](../../../../images/BLADE/image.png)

  - 知识的融合：Bayesian Prompted Optimization 把prompt optimization建模为一个组合优化问题

  ![img2.png](../../../../images/BLADE/image2.png)

### LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models

- motivation: 全参微调（FFT）对算力和存储资源的要求比较高，高效参数微调（PEFT）可以解决这一问题。但是PEFT方法下何时、如何进行超参配置，以及其对不同任务的性能影响仍亟待探究。

- contribution: 探究不同PEFT方法的最佳位置和配置；构建用于不同PEFT方法对比的训练集；将各种adapters集成到LLMs中。

  ![img3.png](../../../../images/Adapter/image3.png)

### LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design

- motivation: 动态适配器允许在微调的过程中动态选择适配器，从而提高模型的适应性。但是在使用基于MOE路由结构时，每一层都需要额外的CUDA内核调用，这导致解码推理延迟增加250-950%。

- novelty: 通过预合并适配器、基于令牌的路由以及高效的CUDA内核设计，提出了一个降低动态适配器延迟的实用方案，从而解决了动态适配器引发的延迟问题。