---
title: 'Paper reading 2'
date: 2024-01-08
permalink: /posts/2024/01/blog-post-2/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### Mutual enhancement of large and small language models with cross-silo knowledge transfer

- motivation: 
  - The training of LLM is computationally, memory-intensive and unsafe. Federated parameter-efficient fine-tuning approaches can protect user privacy but require significant resources[1,2,3]. Training LLMs in a split way decreases the computational requirements but may cause privacy compromisation[4].
  - The inference of LLM faces costs increasement and response delays with the number of users increase. Besides, the exposure of user information is dangerous.
  - Thus method is in urgent need to achieve effective collaborative training between heterogeneous SLMs and the LLM without shared data.

- Novelty: enhance LLM through the feedback of the locally-trained SLM. Further enhance SLM through the synthetic data from LLM. The performance of SLM, natural language understanding performance of LLMs, and natural language generation performance of LLMs are improved.

![img.png](../../../../images/CrossLM/image.png)

- Reflection: The optimization of LLM still depends on the Loss function, which means needs to access the whole model on the cloud. What if the LLM is non-public?

[1] Reduce communication costs and preserve privacy: Prompt tuning method in federated learning.

[2] When federated learning meets pre-trained language models’ parameter-efficient tuning methods.

[3] SLoRA: Federated parameter efficient fine-tuning of language models.

[4] When federated learning meets pre-training.

### Fast Inference from Transformers via Speculative Decoding

- motivation: decoding process in LLMs is slower than in smaller models. And not all the tasks need large models. By decoding easier subtasks in small models in parallel can accelerte inference.

- novelty: using speculative execution method, which means execute tasks only needed. Though speculative sampling, authours maximize the probability of speculative tasks to suggest which task being more needed to be executed.

### FedBERT: When Federated Learning Meets Pre-Training

- motivation: it is hard for clients with limited computing capability to participate in pre-training a large model. Also some raw data information cannot be shared on the server. Taking advantage of the federated learning and split learning approaches can be a solution.

- novelty: Split the most computationally heavy part, the transformer layer in a powerful central server. 
  - Each client trains its local BERT model using its own data and sends the BERT model parameters to the server. Use FedAvg and send global BERT model to all clients.
  ![img2.png](../../../../images/FedBERT/image.png)

  - Utilize federated split learning for pre-trained model is to establish a collaborative learning between the server and the clients.
  ![img3.png](../../../../images/FedBERT/image2.png)
  ![img4.png](../../../../images/FedBERT/image3.png)

- reflection: Split learning calculate the output of BERT model as: 
![img4.png](../../../../images/FedBERT/image4.png)
which means, use the output of the previous layer as the input of the next layer. It is definetion because of the inner order of BERT architecture. To other scenarios, it may make sence to consider the next token.