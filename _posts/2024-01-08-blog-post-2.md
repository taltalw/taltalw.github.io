---
title: 'Paper reading 2'
date: 2024-01-08
permalink: /posts/2024/01/blog-post-2/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### Mutual enhancement of large and small language models with cross-silo knowledge transfer

- motivation: 
  - The training of LLM is computationally, memory-intensive and unsafe. Federated parameter-efficient fine-tuning approaches can protect user privacy but require significant resources[1,2,3]. Training LLMs in a split way decreases the computational requirements but may cause privacy compromisation[4].
  - The inference of LLM faces costs increasement and response delays with the number of users increase. Besides, the exposure of user information is dangerous.
  - Thus method is in urgent need to achieve effective collaborative training between heterogeneous SLMs and the LLM without shared data.

- Novelty: enhance LLM through the feedback of the locally-trained SLM. Further enhance SLM through the synthetic data from LLM. The performance of SLM, natural language understanding performance of LLMs, and natural language generation performance of LLMs are improved.

![img.png](../../../../images/CrossLM/image.png)

- Reflection: The optimization of LLM still depends on the Loss function, which means needs to access the whole model on the cloud. What if the LLM is non-public?

[1] Reduce communication costs and preserve privacy: Prompt tuning method in federated learning.

[2] When federated learning meets pre-trained language models’ parameter-efficient tuning methods.

[3] SLoRA: Federated parameter efficient fine-tuning of language models.

[12] When federated learning meets pre-training.



