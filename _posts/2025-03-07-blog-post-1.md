---
title: 'Paper reading 6'
date: 2025-03-07
permalink: /posts/2025/03/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### EFFICIENTLY LEARNING AT TEST-TIME: ACTIVE FINE-TUNING OF LLMS

- motivation: test-time fine-tuning过程中，邻域检索能够选择相关但可能冗余的数据从而导致不确定性，而主动学习选择多样化但可能不相关的数据。将两者结合起来，可以保证数据的多样性和相关性。

- novelty: 在test-time阶段，通过一个代理模型评估数据的不确定性，然后选择ft的数据，选择的依据是使代理模型的近似误差最小。

![img](../../../../images/20250307/image.png)

### Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning

- motivation: 现有的 关于test time推理方法的缩放行为的研究比较少，且目前的工作在很大程度上提供了负面结果。因此有必要探究 如果允许LLM使用一定的test-time compute，它在具有挑战性的prompt上的表现可以提高多少。

- novelty: 展示扩展test-time compute的优缺点，并将其与扩展pre-training compute相比较。

