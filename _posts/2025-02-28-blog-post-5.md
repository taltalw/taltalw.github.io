---
title: 'Paper reading 5'
date: 2025-02-28
permalink: /posts/2025/02/blog-post-5/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### TRAIN SMALL, INFER LARGE: MEMORY-EFFICIENT LORA TRAINING FOR LARGE LANGUAGE MODELS

- motivation: Lora的内存占用情况取决于原始模型的参数量，且未参与训练的参数占据了相当一部分的内存，能否在训练过程中减少这一部分参数的内存占用呢？

- novelty: 训练一个剪枝后的模型（小模型），只更新“小模型”的低秩矩阵从而避免未参与训练的参数过多地占用内存，然后“恢复”低秩矩阵的维度，使其和原始模型的维度相同，再与原始模型相结合进行推理。

  - 主要通过一个mask进行实现，对哪些参数参与了训练进行标记。维度恢复通过对未参与训练的参数标0，然后就能与原始参数进行叠加。

![img](../../../../images/20250228/image.png)

### Collab: Controlled Decoding using Mixture of Agents for LLM Alignment

- motivation: controlled decoding能够解放RLHF的算力，但是单个agent的decooding难以泛化，而多agent的方法通常依赖于弱监督或固定公式来混合输出，因此不够灵活。能否在推理过程中进行动态地选择和协作？

- novelty: 在每一轮next token prediction的过程中，动态选择LLM agent，通过Q function对动作进行评分，选出评分最高的动作的next token prediction。

![img1](../../../../images/20250228/image-1.png)


### POLICY-AWARE REWARD MODELING WITH UNCERTAINTY-GRADIENT BASED DATA AUGMENTATION

- motivation: 奖励模型在 用RLHF微调LLM 的过程中至关重要。由于策略优化不断改变奖励模型训练数据集的分布，可能会降低奖励模型的性能。收集新的偏好数据进行训练可以解决这一问题。但是这一过程的成本比较高，因此，需要一种新的方法来获取高质量的数据，从而有效地更新奖励模型。

- novelty: 对最不确定（方差大，提升潜力大）和最具影响力（过滤）的交互数据进行重新标记，以进一步完善奖励模型，从而缓解奖励模型偏离分布的问题。

![img2](../../../../images/20250228/image-2.png)

### ARMAP: SCALING AUTONOMOUS AGENTS VIA AUTOMATIC REWARD MODELING AND PLANNING

- motivation: LLM在需要多步决策和依赖于环境的场景下表现不佳，收集多步反馈数据进行微调比较困难。事实证明，评估比生成更简单，目前已有研究将强大的商业LLM api作为任务的评估器。能否训练一个奖励模型作为评估器？

- novelty: 提出一种奖励模型的训练方法，通过（任务意图、积极反应和消极反应）三元组数据（数据也是预先由LLM生成的）来训练奖励模型，从而使其能够在之后的任务中进行动态决策。

![img3](../../../../images/20250228/image-3.png)



- motivation: LoRA高度模块化，易于合并和分离。但是不同数据集上的知识冲突导致合并LoRA模块具有一定的挑战，问题在于该如何合并LoRA模块并且将单个模块在特定领域的性能最大化。

- novelty: 引入Selector，基于kmeans的数据选择方法来训练和更新选择器，以维护适配器各自的功能，并且不需要进行多任务微调。

![img.png](../../../../images/AS/image.png)

![img2.png](../../../../images/AS/image2.png)

### Hybrid SLM and LLM for Edge-Cloud Collaborative Inference

- motivation: 基于模型分割的边云协同方法需要大量的通信成本，基于置信度的方法只能获得特定任务的信任度评分。除了简单的按提示或按任务模型调度之外，token层面的协同也是一个思路，因为有的token可以被视为更容易生成的（SLM），而有些是更难的（LLM），关键在于如何确定哪些token是容易的、哪些是难的。

- novelty: 利用设备上的slm根据生成token，调用llm来验证和纠正阈值门控的“更难”的token，从而在推理质量和成本之间实现可控的权衡。

![img3.png](../../../../images/AS/image3.png)

### Beyond Sample-Level Feedback: Using Reference-Level Feedback to Guide Data Synthesis

- motivation: 使用合成数据创建指令调优数据集能够解决数据集不全和人工标注的成本问题，现有的基于反馈的合成数据优化方法往往是在样本层面进行的，也就是对每个样本分别根据反馈进行优化，这种方法的效率亟待提升。

- novelty: 提出一种reference层面的基于反馈的数据合成方法，不再对单个样本进行一对一反馈优化，而是根据事先提取出的reference，作为反馈批量地进行优化生成。

![image4.png](../../../../images/AS/image4.png)

- instruction feedback: 相关领域+所需技能

- response feedback：对回答的多维度评估
