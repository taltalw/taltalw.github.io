---
title: 'Paper reading 5'
date: 2025-02-28
permalink: /posts/2025/02/blog-post-5/
tags:
  - cool posts
  - category1
  - category2
---

# 论文阅读

### TRAIN SMALL, INFER LARGE: MEMORY-EFFICIENT LORA TRAINING FOR LARGE LANGUAGE MODELS

- motivation: Lora的内存占用情况取决于原始模型的参数量，且未参与训练的参数占据了相当一部分的内存，能否在训练过程中减少这一部分参数的内存占用呢？

- novelty: 训练一个剪枝后的模型（小模型），只更新“小模型”的低秩矩阵从而避免未参与训练的参数过多地占用内存，然后“恢复”低秩矩阵的维度，使其和原始模型的维度相同，再与原始模型相结合进行推理。

  - 主要通过一个mask进行实现，对哪些参数参与了训练进行标记。维度恢复通过对未参与训练的参数标0，然后就能与原始参数进行叠加。

![img](../../../../images/20250228/image.png)

### Collab: Controlled Decoding using Mixture of Agents for LLM Alignment

- motivation: controlled decoding能够解放RLHF的算力，但是单个agent的decooding难以泛化，而多agent的方法通常依赖于弱监督或固定公式来混合输出，因此不够灵活。能否在推理过程中进行动态地选择和协作？

- novelty: 在每一轮next token prediction的过程中，动态选择LLM agent，通过Q function对动作进行评分，选出评分最高的动作的next token prediction。

![img1](../../../../images/20250228/image-1.png)


### POLICY-AWARE REWARD MODELING WITH UNCERTAINTY-GRADIENT BASED DATA AUGMENTATION

- motivation: 奖励模型在 用RLHF微调LLM 的过程中至关重要。由于策略优化不断改变奖励模型训练数据集的分布，可能会降低奖励模型的性能。收集新的偏好数据进行训练可以解决这一问题。但是这一过程的成本比较高，因此，需要一种新的方法来获取高质量的数据，从而有效地更新奖励模型。

- novelty: 对最不确定（方差大，提升潜力大）和最具影响力（过滤）的交互数据进行重新标记，以进一步完善奖励模型，从而缓解奖励模型偏离分布的问题。

![img2](../../../../images/20250228/image-2.png)

### ARMAP: SCALING AUTONOMOUS AGENTS VIA AUTOMATIC REWARD MODELING AND PLANNING

- motivation: LLM在需要多步决策和依赖于环境的场景下表现不佳，收集多步反馈数据进行微调比较困难。事实证明，评估比生成更简单，目前已有研究将强大的商业LLM api作为任务的评估器。能否训练一个奖励模型作为评估器？

- novelty: 提出一种奖励模型的训练方法，通过（任务意图、积极反应和消极反应）三元组数据（数据也是预先由LLM生成的）来训练奖励模型，从而使其能够在之后的任务中进行动态决策。

![img3](../../../../images/20250228/image-3.png)
